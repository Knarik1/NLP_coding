{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import get_tokenizer\n",
    "from typing import List, Iterable\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'de'\n",
    "target_lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[source_lang] = get_tokenizer(tokenizer='spacy', language='de_core_news_sm')\n",
    "token_transform[target_lang] = get_tokenizer(tokenizer='spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': functools.partial(<function _spacy_tokenize at 0x7fedf9cf9e50>, spacy=<spacy.lang.de.German object at 0x7fed11123f10>),\n",
       " 'en': functools.partial(<function _spacy_tokenize at 0x7fedf9cf9e50>, spacy=<spacy.lang.en.English object at 0x7fed12284f40>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {source_lang: 0, target_lang: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Several',\n",
       " 'men',\n",
       " 'in',\n",
       " 'hard',\n",
       " 'hats',\n",
       " 'are',\n",
       " 'operating',\n",
       " 'a',\n",
       " 'giant',\n",
       " 'pulley',\n",
       " 'system',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(yield_tokens(train_iter, 'en'))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [source_lang, target_lang]:\n",
    "    train_iter = Multi30k(split='train')\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, language=ln), min_freq=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[\"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Several',\n",
       " 'men',\n",
       " 'in',\n",
       " 'hard',\n",
       " 'hats',\n",
       " 'are',\n",
       " 'operating',\n",
       " 'a',\n",
       " 'giant',\n",
       " 'pulley',\n",
       " 'system',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(yield_tokens(train_iter, language='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform['de']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 dropout,\n",
    "                 maxlen):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        print(\"den shape\", den.shape)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        print(\"pos shape\", pos.shape)\n",
    "        pos_embedding = torch.zeros(maxlen, emb_size)\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos*den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        print(pos_embedding[:, 3])\n",
    "        print(pos_embedding.shape)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        print(pos_embedding.shape)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "        \n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        x = self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(), 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "den shape torch.Size([150])\n",
      "pos shape torch.Size([25, 1])\n",
      "tensor([ 1.0000,  0.5894, -0.3051, -0.9492, -0.8138, -0.0102,  0.8018,  0.9554,\n",
      "         0.3244, -0.5729, -0.9998, -0.6057,  0.2857,  0.9426,  0.8254,  0.0305,\n",
      "        -0.7895, -0.9612, -0.3436,  0.5561,  0.9992,  0.6218, -0.2662, -0.9356,\n",
      "        -0.8367])\n",
      "torch.Size([25, 300])\n",
      "torch.Size([25, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "p_em = PositionalEmbedding(300, 0.2, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150])\n",
      "tensor([1.0000e+00, 9.4044e-01, 8.8444e-01, 8.3176e-01, 7.8223e-01, 7.3564e-01,\n",
      "        6.9183e-01, 6.5063e-01, 6.1188e-01, 5.7544e-01, 5.4117e-01, 5.0894e-01,\n",
      "        4.7863e-01, 4.5013e-01, 4.2332e-01, 3.9811e-01, 3.7440e-01, 3.5210e-01,\n",
      "        3.3113e-01, 3.1141e-01, 2.9286e-01, 2.7542e-01, 2.5902e-01, 2.4359e-01,\n",
      "        2.2909e-01, 2.1544e-01, 2.0261e-01, 1.9055e-01, 1.7920e-01, 1.6853e-01,\n",
      "        1.5849e-01, 1.4905e-01, 1.4017e-01, 1.3183e-01, 1.2397e-01, 1.1659e-01,\n",
      "        1.0965e-01, 1.0312e-01, 9.6977e-02, 9.1201e-02, 8.5770e-02, 8.0662e-02,\n",
      "        7.5858e-02, 7.1340e-02, 6.7091e-02, 6.3096e-02, 5.9338e-02, 5.5804e-02,\n",
      "        5.2481e-02, 4.9355e-02, 4.6416e-02, 4.3652e-02, 4.1052e-02, 3.8607e-02,\n",
      "        3.6308e-02, 3.4145e-02, 3.2112e-02, 3.0200e-02, 2.8401e-02, 2.6710e-02,\n",
      "        2.5119e-02, 2.3623e-02, 2.2216e-02, 2.0893e-02, 1.9649e-02, 1.8478e-02,\n",
      "        1.7378e-02, 1.6343e-02, 1.5370e-02, 1.4454e-02, 1.3594e-02, 1.2784e-02,\n",
      "        1.2023e-02, 1.1307e-02, 1.0633e-02, 1.0000e-02, 9.4044e-03, 8.8444e-03,\n",
      "        8.3176e-03, 7.8223e-03, 7.3564e-03, 6.9183e-03, 6.5063e-03, 6.1188e-03,\n",
      "        5.7544e-03, 5.4117e-03, 5.0894e-03, 4.7863e-03, 4.5013e-03, 4.2332e-03,\n",
      "        3.9811e-03, 3.7440e-03, 3.5210e-03, 3.3113e-03, 3.1141e-03, 2.9286e-03,\n",
      "        2.7542e-03, 2.5902e-03, 2.4359e-03, 2.2909e-03, 2.1544e-03, 2.0261e-03,\n",
      "        1.9055e-03, 1.7920e-03, 1.6853e-03, 1.5849e-03, 1.4905e-03, 1.4017e-03,\n",
      "        1.3183e-03, 1.2397e-03, 1.1659e-03, 1.0965e-03, 1.0312e-03, 9.6977e-04,\n",
      "        9.1201e-04, 8.5770e-04, 8.0662e-04, 7.5858e-04, 7.1340e-04, 6.7091e-04,\n",
      "        6.3096e-04, 5.9338e-04, 5.5804e-04, 5.2481e-04, 4.9355e-04, 4.6416e-04,\n",
      "        4.3652e-04, 4.1052e-04, 3.8607e-04, 3.6308e-04, 3.4145e-04, 3.2112e-04,\n",
      "        3.0200e-04, 2.8401e-04, 2.6710e-04, 2.5119e-04, 2.3623e-04, 2.2216e-04,\n",
      "        2.0893e-04, 1.9649e-04, 1.8479e-04, 1.7378e-04, 1.6343e-04, 1.5370e-04,\n",
      "        1.4454e-04, 1.3594e-04, 1.2784e-04, 1.2023e-04, 1.1307e-04, 1.0633e-04])\n"
     ]
    }
   ],
   "source": [
    "v = torch.exp(- torch.arange(0, 300, 2)* math.log(10000) / 300)\n",
    "print(v.shape)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976184"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,  26,\n",
       "         28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,  52,  54,\n",
       "         56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,  78,  80,  82,\n",
       "         84,  86,  88,  90,  92,  94,  96,  98, 100, 102, 104, 106, 108, 110,\n",
       "        112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138,\n",
       "        140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166,\n",
       "        168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194,\n",
       "        196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222,\n",
       "        224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250,\n",
       "        252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278,\n",
       "        280, 282, 284, 286, 288, 290, 292, 294, 296, 298])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, 300, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030701134573253946"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(10000)/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9996.59685943787"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.e**9.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,11,22], [3,4, 33, 44]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 11],\n",
       "       [ 3, 33]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int) -> None:\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(x.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
